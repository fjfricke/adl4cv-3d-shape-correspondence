
@article{ren_grounded_2024,
	title = {Grounded {SAM}: {Assembling} {Open}-{World} {Models} for {Diverse} {Visual} {Tasks}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Grounded {SAM}},
	url = {https://arxiv.org/abs/2401.14159},
	doi = {10.48550/ARXIV.2401.14159},
	abstract = {We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models. As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline. For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything. Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis. Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models.},
	urldate = {2024-10-30},
	author = {Ren, Tianhe and Liu, Shilong and Zeng, Ailing and Lin, Jing and Li, Kunchang and Cao, He and Chen, Jiayu and Huang, Xinyu and Chen, Yukang and Yan, Feng and Zeng, Zhaoyang and Zhang, Hao and Li, Feng and Yang, Jie and Li, Hongyang and Jiang, Qing and Zhang, Lei},
	year = {2024},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
}

@misc{ravi_sam_2024,
	title = {{SAM} 2: {Segment} {Anything} in {Images} and {Videos}},
	shorttitle = {{SAM} 2},
	url = {http://arxiv.org/abs/2408.00714},
	doi = {10.48550/arXiv.2408.00714},
	abstract = {We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and Rädle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Dollár, Piotr and Feichtenhofer, Christoph},
	month = oct,
	year = {2024},
	note = {arXiv:2408.00714},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{ravi_sam_nodate,
	title = {{SAM} 2: {Segment} {Anything} in {Images} and {Videos}},
	language = {en},
	author = {Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and Rädle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Dollár, Piotr and Feichtenhofer, Christoph},
}

@inproceedings{noauthor_sam_2024,
	title = {{SAM} 2: {Segment} {Anything} in {Images} and {Videos}},
	shorttitle = {{SAM} 2},
	url = {https://openreview.net/forum?id=Ha6RTeWMd0},
	abstract = {We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, the dataset, an interactive demo and code.},
	language = {en},
	urldate = {2024-10-30},
	month = oct,
	year = {2024},
}

@article{liu_grounding_2023,
	title = {Grounding {DINO}: {Marrying} {DINO} with {Grounded} {Pre}-{Training} for {Open}-{Set} {Object} {Detection}},
	shorttitle = {Grounding {DINO}},
	url = {https://openreview.net/forum?id=DS5qRs0tQz},
	abstract = {In this paper, we develop an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a \$52.5\$ AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean \$26.1\$ AP.},
	language = {en},
	urldate = {2024-10-30},
	author = {Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and Zhang, Lei},
	month = oct,
	year = {2023},
}

@inproceedings{liu_partslip_2023,
	title = {{PartSLIP}: {Low}-{Shot} {Part} {Segmentation} for {3D} {Point} {Clouds} via {Pretrained} {Image}-{Language} {Models}},
	shorttitle = {{PartSLIP}},
	doi = {10.1109/CVPR52729.2023.02082},
	author = {Liu, Minghua and Zhu, Yinhao and Cai, Hong and Han, Shizhong and Ling, Zhan and Porikli, Fatih and Su, Hao},
	month = jun,
	year = {2023},
	pages = {21736--21746},
}

@article{kirillov_segment_2023,
	title = {Segment {Anything}},
	copyright = {https://doi.org/10.15223/policy-029},
	url = {https://ieeexplore.ieee.org/document/10378323/},
	doi = {10.1109/ICCV51070.2023.00371},
	abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive – often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at segment-anything.com to foster research into foundation models for computer vision. We recommend reading the full paper at: arxiv.org/abs/2304.02643.},
	urldate = {2024-10-30},
	journal = {2023 IEEE/CVF International Conference on Computer Vision (ICCV)},
	author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
	month = oct,
	year = {2023},
	note = {Conference Name: 2023 IEEE/CVF International Conference on Computer Vision (ICCV)
ISBN: 9798350307184
Place: Paris, France
Publisher: IEEE},
	pages = {3992--4003},
}

@misc{guo_sam2point_2024,
	title = {{SAM2Point}: {Segment} {Any} {3D} as {Videos} in {Zero}-shot and {Promptable} {Manners}},
	shorttitle = {{SAM2Point}},
	url = {http://arxiv.org/abs/2408.16768},
	abstract = {We introduce SAM2Point, a preliminary exploration adapting Segment Anything Model 2 (SAM 2) for zero-shot and promptable 3D segmentation. SAM2Point interprets any 3D data as a series of multi-directional videos, and leverages SAM 2 for 3D-space segmentation, without further training or 2D-3D projection. Our framework supports various prompt types, including 3D points, boxes, and masks, and can generalize across diverse scenarios, such as 3D objects, indoor scenes, outdoor environments, and raw sparse LiDAR. Demonstrations on multiple 3D datasets, e.g., Objaverse, S3DIS, ScanNet, Semantic3D, and KITTI, highlight the robust generalization capabilities of SAM2Point. To our best knowledge, we present the most faithful implementation of SAM in 3D, which may serve as a starting point for future research in promptable 3D segmentation. Online Demo: https://huggingface.co/spaces/ZiyuG/SAM2Point . Code: https://github.com/ZiyuGuo99/SAM2Point .},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Guo, Ziyu and Zhang, Renrui and Zhu, Xiangyang and Tong, Chengzhuo and Gao, Peng and Li, Chunyuan and Heng, Pheng-Ann},
	month = aug,
	year = {2024},
	note = {arXiv:2408.16768},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@article{guo_sam2point_2024-1,
	title = {{SAM2Point}: {Segment} {Any} {3D} as {Videos} in {Zero}-shot and {Promptable} {Manners}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{SAM2Point}},
	url = {https://arxiv.org/abs/2408.16768},
	doi = {10.48550/ARXIV.2408.16768},
	abstract = {We introduce SAM2Point, a preliminary exploration adapting Segment Anything Model 2 (SAM 2) for zero-shot and promptable 3D segmentation. SAM2Point interprets any 3D data as a series of multi-directional videos, and leverages SAM 2 for 3D-space segmentation, without further training or 2D-3D projection. Our framework supports various prompt types, including 3D points, boxes, and masks, and can generalize across diverse scenarios, such as 3D objects, indoor scenes, outdoor environments, and raw sparse LiDAR. Demonstrations on multiple 3D datasets, e.g., Objaverse, S3DIS, ScanNet, Semantic3D, and KITTI, highlight the robust generalization capabilities of SAM2Point. To our best knowledge, we present the most faithful implementation of SAM in 3D, which may serve as a starting point for future research in promptable 3D segmentation. Online Demo: https://huggingface.co/spaces/ZiyuG/SAM2Point . Code: https://github.com/ZiyuGuo99/SAM2Point .},
	urldate = {2024-10-30},
	author = {Guo, Ziyu and Zhang, Renrui and Zhu, Xiangyang and Tong, Chengzhuo and Gao, Peng and Li, Chunyuan and Heng, Pheng-Ann},
	year = {2024},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
}

@book{guo_sam2point_2024-2,
	title = {{SAM2Point}: {Segment} {Any} {3D} as {Videos} in {Zero}-shot and {Promptable} {Manners}},
	shorttitle = {{SAM2Point}},
	abstract = {We introduce SAM2Point, a preliminary exploration adapting Segment Anything Model 2 (SAM 2) for zero-shot and promptable 3D segmentation. SAM2Point interprets any 3D data as a series of multi-directional videos, and leverages SAM 2 for 3D-space segmentation, without further training or 2D-3D projection. Our framework supports various prompt types, including 3D points, boxes, and masks, and can generalize across diverse scenarios, such as 3D objects, indoor scenes, outdoor environments, and raw sparse LiDAR. Demonstrations on multiple 3D datasets, e.g., Objaverse, S3DIS, ScanNet, Semantic3D, and KITTI, highlight the robust generalization capabilities of SAM2Point. To our best knowledge, we present the most faithful implementation of SAM in 3D, which may serve as a starting point for future research in promptable 3D segmentation. Online Demo: https://huggingface.co/spaces/ZiyuG/SAM2Point . Code: https://github.com/ZiyuGuo99/SAM2Point .},
	author = {Guo, Ziyu and Zhang, Renrui and Zhu, Xiangyang and Tong, Chengzhuo and Gao, Peng and Li, Chunyuan and Heng, Pheng-Ann},
	month = aug,
	year = {2024},
	doi = {10.48550/arXiv.2408.16768},
}

@inproceedings{abdelreheem_satr_2023,
	title = {{SATR}: {Zero}-{Shot} {Semantic} {Segmentation} of {3D} {Shapes}},
	shorttitle = {{SATR}},
	doi = {10.1109/ICCV51070.2023.01392},
	author = {Abdelreheem, Ahmed and Skorokhodov, Ivan and Ovsjanikov, Maks and Wonka, Peter},
	month = oct,
	year = {2023},
	pages = {15120--15133},
}

@inproceedings{abdelreheem_zero-shot_2023,
	address = {Sydney NSW Australia},
	title = {Zero-{Shot} {3D} {Shape} {Correspondence}},
	isbn = {9798400703157},
	url = {https://dl.acm.org/doi/10.1145/3610548.3618228},
	doi = {10.1145/3610548.3618228},
	language = {en},
	urldate = {2024-10-30},
	booktitle = {{SIGGRAPH} {Asia} 2023 {Conference} {Papers}},
	publisher = {ACM},
	author = {Abdelreheem, Ahmed and Eldesokey, Abdelrahman and Ovsjanikov, Maks and Wonka, Peter},
	month = dec,
	year = {2023},
	pages = {1--11},
}

@inproceedings{halimi_unsupervised_2019,
	title = {Unsupervised {Learning} of {Dense} {Shape} {Correspondence}},
	url = {https://ieeexplore.ieee.org/document/8953366},
	doi = {10.1109/CVPR.2019.00450},
	abstract = {We introduce the first completely unsupervised correspondence learning approach for deformable 3D shapes. Key to our model is the understanding that natural deformations (such as changes in pose) approximately preserve the metric structure of the surface, yielding a natural criterion to drive the learning process toward distortion-minimizing predictions. On this basis, we overcome the need for annotated data and replace it by a purely geometric criterion. The resulting learning model is class-agnostic, and is able to leverage any type of deformable geometric data for the training phase. In contrast to existing supervised approaches which specialize on the class seen at training time, we demonstrate stronger generalization as well as applicability to a variety of challenging settings. We showcase our method on a wide selection of correspondence benchmarks, where we outperform other methods in terms of accuracy, generalization, and efficiency.},
	urldate = {2024-10-30},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Halimi, Oshri and Litany, Or and Rodolà, Emanuele Rodolà and Bronstein, Alex M. and Kimmel, Ron},
	month = jun,
	year = {2019},
	note = {ISSN: 2575-7075},
	keywords = {Categorization, Computer Vision Theory, Deep Learning, Grouping an, Recognition: Detection, Retrieval, Segmentation},
	pages = {4365--4374},
}

@inproceedings{halimi_unsupervised_2019-1,
	title = {Unsupervised {Learning} of {Dense} {Shape} {Correspondence}},
	url = {https://ieeexplore.ieee.org/document/8953366},
	doi = {10.1109/CVPR.2019.00450},
	abstract = {We introduce the first completely unsupervised correspondence learning approach for deformable 3D shapes. Key to our model is the understanding that natural deformations (such as changes in pose) approximately preserve the metric structure of the surface, yielding a natural criterion to drive the learning process toward distortion-minimizing predictions. On this basis, we overcome the need for annotated data and replace it by a purely geometric criterion. The resulting learning model is class-agnostic, and is able to leverage any type of deformable geometric data for the training phase. In contrast to existing supervised approaches which specialize on the class seen at training time, we demonstrate stronger generalization as well as applicability to a variety of challenging settings. We showcase our method on a wide selection of correspondence benchmarks, where we outperform other methods in terms of accuracy, generalization, and efficiency.},
	urldate = {2024-10-30},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Halimi, Oshri and Litany, Or and Rodolà, Emanuele Rodolà and Bronstein, Alex M. and Kimmel, Ron},
	month = jun,
	year = {2019},
	note = {ISSN: 2575-7075},
	keywords = {Categorization, Computer Vision Theory, Deep Learning, Grouping an, Recognition: Detection, Retrieval, Segmentation},
	pages = {4365--4374},
}

@inproceedings{zuffi_3d_2017,
	address = {Honolulu, HI},
	title = {{3D} {Menagerie}: {Modeling} the {3D} {Shape} and {Pose} of {Animals}},
	isbn = {978-1-5386-0457-1},
	shorttitle = {{3D} {Menagerie}},
	url = {http://ieeexplore.ieee.org/document/8100069/},
	doi = {10.1109/CVPR.2017.586},
	abstract = {There has been signiﬁcant work on learning realistic, articulated, 3D models of the human body. In contrast, there are few such models of animals, despite many applications. The main challenge is that animals are much less cooperative than humans. The best human body models are learned from thousands of 3D scans of people in speciﬁc poses, which is infeasible with live animals. Consequently, we learn our model from a small set of 3D scans of toy ﬁgurines in arbitrary poses. We employ a novel partbased shape model to compute an initial registration to the scans. We then normalize their pose, learn a statistical shape model, and reﬁne the registrations and the model together. In this way, we accurately align animal scans from different quadruped families with very different shapes and poses. With the registration to a common template we learn a shape space representing animals including lions, cats, dogs, horses, cows and hippos. Animal shapes can be sampled from the model, posed, animated, and ﬁt to data. We demonstrate generalization by ﬁtting it to images of real animals including species not seen in training.},
	language = {en},
	urldate = {2024-10-30},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zuffi, Silvia and Kanazawa, Angjoo and Jacobs, David W. and Black, Michael J.},
	month = jul,
	year = {2017},
	pages = {5524--5532},
}

@inproceedings{bogo_faust_2014,
	address = {Columbus, OH, USA},
	title = {{FAUST}: {Dataset} and {Evaluation} for {3D} {Mesh} {Registration}},
	isbn = {978-1-4799-5118-5},
	shorttitle = {{FAUST}},
	url = {https://ieeexplore.ieee.org/document/6909880},
	doi = {10.1109/CVPR.2014.491},
	abstract = {New scanning technologies are increasing the importance of 3D mesh data and the need for algorithms that can reliably align it. Surface registration is important for building full 3D models from partial scans, creating statistical shape models, shape retrieval, and tracking. The problem is particularly challenging for non-rigid and articulated objects like human bodies. While the challenges of real-world data registration are not present in existing synthetic datasets, establishing ground-truth correspondences for real 3D scans is difﬁcult. We address this with a novel mesh registration technique that combines 3D shape and appearance information to produce high-quality alignments. We deﬁne a new dataset called FAUST that contains 300 scans of 10 people in a wide range of poses together with an evaluation methodology. To achieve accurate registration, we paint the subjects with high-frequency textures and use an extensive validation process to ensure accurate ground truth. We ﬁnd that current shape registration methods have trouble with this real-world data. The dataset and evaluation website are available for research purposes at http://faust.is.tue.mpg.de.},
	language = {en},
	urldate = {2024-10-30},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Bogo, Federica and Romero, Javier and Loper, Matthew and Black, Michael J.},
	month = jun,
	year = {2014},
	pages = {3794--3801},
}

@misc{ren_grounding_2024,
	title = {Grounding {DINO} 1.5: {Advance} the "{Edge}" of {Open}-{Set} {Object} {Detection}},
	shorttitle = {Grounding {DINO} 1.5},
	url = {http://arxiv.org/abs/2405.10300},
	doi = {10.48550/arXiv.2405.10300},
	abstract = {This paper introduces Grounding DINO 1.5, a suite of advanced open-set object detection models developed by IDEA Research, which aims to advance the "Edge" of open-set object detection. The suite encompasses two models: Grounding DINO 1.5 Pro, a high-performance model designed for stronger generalization capability across a wide range of scenarios, and Grounding DINO 1.5 Edge, an efficient model optimized for faster speed demanded in many applications requiring edge deployment. The Grounding DINO 1.5 Pro model advances its predecessor by scaling up the model architecture, integrating an enhanced vision backbone, and expanding the training dataset to over 20 million images with grounding annotations, thereby achieving a richer semantic understanding. The Grounding DINO 1.5 Edge model, while designed for efficiency with reduced feature scales, maintains robust detection capabilities by being trained on the same comprehensive dataset. Empirical results demonstrate the effectiveness of Grounding DINO 1.5, with the Grounding DINO 1.5 Pro model attaining a 54.3 AP on the COCO detection benchmark and a 55.7 AP on the LVIS-minival zero-shot transfer benchmark, setting new records for open-set object detection. Furthermore, the Grounding DINO 1.5 Edge model, when optimized with TensorRT, achieves a speed of 75.2 FPS while attaining a zero-shot performance of 36.2 AP on the LVIS-minival benchmark, making it more suitable for edge computing scenarios. Model examples and demos with API will be released at https://github.com/IDEA-Research/Grounding-DINO-1.5-API},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Ren, Tianhe and Jiang, Qing and Liu, Shilong and Zeng, Zhaoyang and Liu, Wenlong and Gao, Han and Huang, Hongjie and Ma, Zhengyu and Jiang, Xiaoke and Chen, Yihao and Xiong, Yuda and Zhang, Hao and Li, Feng and Tang, Peijun and Yu, Kent and Zhang, Lei},
	month = jun,
	year = {2024},
	note = {arXiv:2405.10300},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{cen_segment_2023,
	title = {Segment {Anything} in {3D} with {NeRFs}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/525d24400247f884c3419b0b7b1c4829-Abstract-Conference.html},
	language = {en},
	urldate = {2024-10-30},
	journal = {Advances in Neural Information Processing Systems},
	author = {Cen, Jiazhong and Zhou, Zanwei and Fang, Jiemin and Yang, Chen and Shen, Wei and Xie, Lingxi and Jiang, Dongsheng and Zhang, Xiaopeng and Tian, Qi},
	month = dec,
	year = {2023},
	pages = {25971--25990},
}

@book{liu_partslip_2022,
	title = {{PartSLIP}: {Low}-{Shot} {Part} {Segmentation} for {3D} {Point} {Clouds} via {Pretrained} {Image}-{Language} {Models}},
	shorttitle = {{PartSLIP}},
	abstract = {Generalizable 3D part segmentation is important but challenging in vision and robotics. Training deep models via conventional supervised methods requires large-scale 3D datasets with fine-grained part annotations, which are costly to collect. This paper explores an alternative way for low-shot part segmentation of 3D point clouds by leveraging a pretrained image-language model, GLIP, which achieves superior performance on open-vocabulary 2D detection. We transfer the rich knowledge from 2D to 3D through GLIP-based part detection on point cloud rendering and a novel 2D-to-3D label lifting algorithm. We also utilize multi-view 3D priors and few-shot prompt tuning to boost performance significantly. Extensive evaluation on PartNet and PartNet-Mobility datasets shows that our method enables excellent zero-shot 3D part segmentation. Our few-shot version not only outperforms existing few-shot approaches by a large margin but also achieves highly competitive results compared to the fully supervised counterpart. Furthermore, we demonstrate that our method can be directly applied to iPhone-scanned point clouds without significant domain gaps.},
	author = {Liu, Minghua and Zhu, Yinhao and Cai, Hong and Han, Shizhong and Ling, Zhan and Porikli, Fatih and Su, Hao},
	month = dec,
	year = {2022},
	doi = {10.48550/arXiv.2212.01558},
}

@misc{yang_sam3d_2023,
	title = {{SAM3D}: {Segment} {Anything} in {3D} {Scenes}},
	shorttitle = {{SAM3D}},
	url = {http://arxiv.org/abs/2306.03908},
	doi = {10.48550/arXiv.2306.03908},
	abstract = {In this work, we propose SAM3D, a novel framework that is able to predict masks in 3D point clouds by leveraging the Segment-Anything Model (SAM) in RGB images without further training or finetuning. For a point cloud of a 3D scene with posed RGB images, we first predict segmentation masks of RGB images with SAM, and then project the 2D masks into the 3D points. Later, we merge the 3D masks iteratively with a bottom-up merging approach. At each step, we merge the point cloud masks of two adjacent frames with the bidirectional merging approach. In this way, the 3D masks predicted from different frames are gradually merged into the 3D masks of the whole 3D scene. Finally, we can optionally ensemble the result from our SAM3D with the over-segmentation results based on the geometric information of the 3D scenes. Our approach is experimented with ScanNet dataset and qualitative results demonstrate that our SAM3D achieves reasonable and fine-grained 3D segmentation results without any training or finetuning of SAM.},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Yang, Yunhan and Wu, Xiaoyang and He, Tong and Zhao, Hengshuang and Liu, Xihui},
	month = jun,
	year = {2023},
	note = {arXiv:2306.03908},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wasim_video-groundingdino_2024,
	title = {Video-{GroundingDINO}: {Towards} {Open}-{Vocabulary} {Spatio}-{Temporal} {Video} {Grounding}},
	shorttitle = {Video-{GroundingDINO}},
	url = {http://arxiv.org/abs/2401.00901},
	doi = {10.48550/arXiv.2401.00901},
	abstract = {Video grounding aims to localize a spatio-temporal section in a video corresponding to an input text query. This paper addresses a critical limitation in current video grounding methodologies by introducing an Open-Vocabulary Spatio-Temporal Video Grounding task. Unlike prevalent closed-set approaches that struggle with open-vocabulary scenarios due to limited training data and predefined vocabularies, our model leverages pre-trained representations from foundational spatial grounding models. This empowers it to effectively bridge the semantic gap between natural language and diverse visual content, achieving strong performance in closed-set and open-vocabulary settings. Our contributions include a novel spatio-temporal video grounding model, surpassing state-of-the-art results in closed-set evaluations on multiple datasets and demonstrating superior performance in open-vocabulary scenarios. Notably, the proposed model outperforms state-of-the-art methods in closed-set settings on VidSTG (Declarative and Interrogative) and HC-STVG (V1 and V2) datasets. Furthermore, in open-vocabulary evaluations on HC-STVG V1 and YouCook-Interactions, our model surpasses the recent best-performing models by \$4.88\$ m\_vIoU and \$1.83{\textbackslash}\%\$ accuracy, demonstrating its efficacy in handling diverse linguistic and visual concepts for improved video understanding. Our codes will be publicly released.},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Wasim, Syed Talal and Naseer, Muzammal and Khan, Salman and Yang, Ming-Hsuan and Khan, Fahad Shahbaz},
	month = mar,
	year = {2024},
	note = {arXiv:2401.00901},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ravi_sam_2024-1,
	title = {{SAM} 2: {Segment} {Anything} in {Images} and {Videos}},
	shorttitle = {{SAM} 2},
	url = {http://arxiv.org/abs/2408.00714},
	doi = {10.48550/arXiv.2408.00714},
	abstract = {We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and Rädle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Dollár, Piotr and Feichtenhofer, Christoph},
	month = oct,
	year = {2024},
	note = {arXiv:2408.00714},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{li_grounded_2022,
	title = {Grounded {Language}-{Image} {Pre}-training},
	url = {http://arxiv.org/abs/2112.03857},
	abstract = {This paper presents a grounded language-image pre-training (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and phrase grounding for pre-training. The unification brings two benefits: 1) it allows GLIP to learn from both detection and grounding data to improve both tasks and bootstrap a good grounding model; 2) GLIP can leverage massive image-text pairs by generating grounding boxes in a self-training fashion, making the learned representation semantic-rich. In our experiments, we pre-train GLIP on 27M grounding data, including 3M human-annotated and 24M web-crawled image-text pairs. The learned representations demonstrate strong zero-shot and few-shot transferability to various object-level recognition tasks. 1) When directly evaluated on COCO and LVIS (without seeing any images in COCO during pre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many supervised baselines. 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13 downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised Dynamic Head. Code is released at https://github.com/microsoft/GLIP.},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and Chang, Kai-Wei and Gao, Jianfeng},
	month = jun,
	year = {2022},
	note = {arXiv:2112.03857},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multimedia},
}

@misc{liu_grounding_2024,
	title = {Grounding {DINO}: {Marrying} {DINO} with {Grounded} {Pre}-{Training} for {Open}-{Set} {Object} {Detection}},
	shorttitle = {Grounding {DINO}},
	url = {http://arxiv.org/abs/2303.05499},
	doi = {10.48550/arXiv.2303.05499},
	abstract = {In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a \$52.5\$ AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean \$26.1\$ AP. Code will be available at {\textbackslash}url\{https://github.com/IDEA-Research/GroundingDINO\}.},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Jiang, Qing and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and Zhang, Lei},
	month = jul,
	year = {2024},
	note = {arXiv:2303.05499},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ren_grounded_2024-1,
	title = {Grounded {SAM}: {Assembling} {Open}-{World} {Models} for {Diverse} {Visual} {Tasks}},
	shorttitle = {Grounded {SAM}},
	url = {http://arxiv.org/abs/2401.14159},
	doi = {10.48550/arXiv.2401.14159},
	abstract = {We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models. As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline. For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything. Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis. Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models.},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Ren, Tianhe and Liu, Shilong and Zeng, Ailing and Lin, Jing and Li, Kunchang and Cao, He and Chen, Jiayu and Huang, Xinyu and Chen, Yukang and Yan, Feng and Zeng, Zhaoyang and Zhang, Hao and Li, Feng and Yang, Jie and Li, Hongyang and Jiang, Qing and Zhang, Lei},
	month = jan,
	year = {2024},
	note = {arXiv:2401.14159},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@book{sun_efficient_2024,
	title = {On {Efficient} {Variants} of {Segment} {Anything} {Model}: {A} {Survey}},
	shorttitle = {On {Efficient} {Variants} of {Segment} {Anything} {Model}},
	abstract = {The Segment Anything Model (SAM) is a foundational model for image segmentation tasks, known for its strong generalization across diverse applications. However, its impressive performance comes with significant computational and resource demands, making it challenging to deploy in resource-limited environments such as mobile devices. To address this, a variety of SAM variants have been proposed to enhance efficiency without sacrificing accuracy. This survey provides the first comprehensive review of these efficient SAM variants. We begin by exploring the motivations driving this research. We then present core techniques used in SAM and model acceleration. This is followed by an in-depth analysis of various acceleration strategies, categorized by approach. Finally, we offer a unified and extensive evaluation of these methods, assessing their efficiency and accuracy on representative benchmarks, and providing a clear comparison of their overall performance.},
	author = {Sun, Xiaorui and Liu, Jun and Shen, Heng and Zhu, Xiaofeng and Hu, Ping},
	month = oct,
	year = {2024},
	doi = {10.48550/arXiv.2410.04960},
}

@misc{shen_fastsam3d_2024,
	title = {{FastSAM3D}: {An} {Efficient} {Segment} {Anything} {Model} for {3D} {Volumetric} {Medical} {Images}},
	shorttitle = {{FastSAM3D}},
	url = {http://arxiv.org/abs/2403.09827},
	doi = {10.48550/arXiv.2403.09827},
	abstract = {Segment anything models (SAMs) are gaining attention for their zero-shot generalization capability in segmenting objects of unseen classes and in unseen domains when properly prompted. Interactivity is a key strength of SAMs, allowing users to iteratively provide prompts that specify objects of interest to refine outputs. However, to realize the interactive use of SAMs for 3D medical imaging tasks, rapid inference times are necessary. High memory requirements and long processing delays remain constraints that hinder the adoption of SAMs for this purpose. Specifically, while 2D SAMs applied to 3D volumes contend with repetitive computation to process all slices independently, 3D SAMs suffer from an exponential increase in model parameters and FLOPS. To address these challenges, we present FastSAM3D which accelerates SAM inference to 8 milliseconds per 128*128*128 3D volumetric image on an NVIDIA A100 GPU. This speedup is accomplished through 1) a novel layer-wise progressive distillation scheme that enables knowledge transfer from a complex 12-layer ViT-B to a lightweight 6-layer ViT-Tiny variant encoder without training from scratch; and 2) a novel 3D sparse flash attention to replace vanilla attention operators, substantially reducing memory needs and improving parallelization. Experiments on three diverse datasets reveal that FastSAM3D achieves a remarkable speedup of 527.38x compared to 2D SAMs and 8.75x compared to 3D SAMs on the same volumes without significant performance decline. Thus, FastSAM3D opens the door for low-cost truly interactive SAM-based 3D medical imaging segmentation with commonly used GPU hardware. Code is available at https://github.com/arcadelab/FastSAM3D.},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Shen, Yiqing and Li, Jingxing and Shao, Xinyuan and Romillo, Blanca Inigo and Jindal, Ankush and Dreizin, David and Unberath, Mathias},
	month = mar,
	year = {2024},
	note = {arXiv:2403.09827},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{guo_sam2point_2024-3,
	title = {{SAM2Point}: {Segment} {Any} {3D} as {Videos} in {Zero}-shot and {Promptable} {Manners}},
	shorttitle = {{SAM2Point}},
	url = {http://arxiv.org/abs/2408.16768},
	abstract = {We introduce SAM2Point, a preliminary exploration adapting Segment Anything Model 2 (SAM 2) for zero-shot and promptable 3D segmentation. SAM2Point interprets any 3D data as a series of multi-directional videos, and leverages SAM 2 for 3D-space segmentation, without further training or 2D-3D projection. Our framework supports various prompt types, including 3D points, boxes, and masks, and can generalize across diverse scenarios, such as 3D objects, indoor scenes, outdoor environments, and raw sparse LiDAR. Demonstrations on multiple 3D datasets, e.g., Objaverse, S3DIS, ScanNet, Semantic3D, and KITTI, highlight the robust generalization capabilities of SAM2Point. To our best knowledge, we present the most faithful implementation of SAM in 3D, which may serve as a starting point for future research in promptable 3D segmentation. Online Demo: https://huggingface.co/spaces/ZiyuG/SAM2Point . Code: https://github.com/ZiyuGuo99/SAM2Point .},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Guo, Ziyu and Zhang, Renrui and Zhu, Xiangyang and Tong, Chengzhuo and Gao, Peng and Li, Chunyuan and Heng, Pheng-Ann},
	month = aug,
	year = {2024},
	note = {arXiv:2408.16768},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{tang_segment_2024,
	title = {Segment {Any} {Mesh}: {Zero}-shot {Mesh} {Part} {Segmentation} via {Lifting} {Segment} {Anything} 2 to {3D}},
	shorttitle = {Segment {Any} {Mesh}},
	url = {http://arxiv.org/abs/2408.13679},
	doi = {10.48550/arXiv.2408.13679},
	abstract = {We propose Segment Any Mesh (SAMesh), a novel zero-shot method for mesh part segmentation that overcomes the limitations of shape analysis-based, learning-based, and current zero-shot approaches. SAMesh operates in two phases: multimodal rendering and 2D-to-3D lifting. In the first phase, multiview renders of the mesh are individually processed through Segment Anything 2 (SAM2) to generate 2D masks. These masks are then lifted into a mesh part segmentation by associating masks that refer to the same mesh part across the multiview renders. We find that applying SAM2 to multimodal feature renders of normals and shape diameter scalars achieves better results than using only untextured renders of meshes. By building our method on top of SAM2, we seamlessly inherit any future improvements made to 2D segmentation. We compare our method with a robust, well-evaluated shape analysis method, Shape Diameter Function (ShapeDiam), and show our method is comparable to or exceeds its performance. Since current benchmarks contain limited object diversity, we also curate and release a dataset of generated meshes and use it to demonstrate our method's improved generalization over ShapeDiam via human evaluation. We release the code and dataset at https://github.com/gtangg12/samesh},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Tang, George and Zhao, William and Ford, Logan and Benhaim, David and Zhang, Paul},
	month = aug,
	year = {2024},
	note = {arXiv:2408.13679},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{kirillov_segment_2023-1,
	title = {Segment {Anything}},
	url = {http://arxiv.org/abs/2304.02643},
	doi = {10.48550/arXiv.2304.02643},
	abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
	urldate = {2024-10-27},
	publisher = {arXiv},
	author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
	month = apr,
	year = {2023},
	note = {arXiv:2304.02643},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{abdelreheem_zero-shot_2023-1,
	title = {Zero-{Shot} {3D} {Shape} {Correspondence}},
	url = {http://arxiv.org/abs/2306.03253},
	doi = {10.48550/arXiv.2306.03253},
	abstract = {We propose a novel zero-shot approach to computing correspondences between 3D shapes. Existing approaches mainly focus on isometric and near-isometric shape pairs (e.g., human vs. human), but less attention has been given to strongly non-isometric and inter-class shape matching (e.g., human vs. cow). To this end, we introduce a fully automatic method that exploits the exceptional reasoning capabilities of recent foundation models in language and vision to tackle difficult shape correspondence problems. Our approach comprises multiple stages. First, we classify the 3D shapes in a zero-shot manner by feeding rendered shape views to a language-vision model (e.g., BLIP2) to generate a list of class proposals per shape. These proposals are unified into a single class per shape by employing the reasoning capabilities of ChatGPT. Second, we attempt to segment the two shapes in a zero-shot manner, but in contrast to the co-segmentation problem, we do not require a mutual set of semantic regions. Instead, we propose to exploit the in-context learning capabilities of ChatGPT to generate two different sets of semantic regions for each shape and a semantic mapping between them. This enables our approach to match strongly non-isometric shapes with significant differences in geometric structure. Finally, we employ the generated semantic mapping to produce coarse correspondences that can further be refined by the functional maps framework to produce dense point-to-point maps. Our approach, despite its simplicity, produces highly plausible results in a zero-shot manner, especially between strongly non-isometric shapes. Project webpage: https://samir55.github.io/3dshapematch/.},
	urldate = {2024-10-25},
	publisher = {arXiv},
	author = {Abdelreheem, Ahmed and Eldesokey, Abdelrahman and Ovsjanikov, Maks and Wonka, Peter},
	month = sep,
	year = {2023},
	note = {arXiv:2306.03253},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ranzinger_am-radio_2024,
	title = {{AM}-{RADIO}: {Agglomerative} {Vision} {Foundation} {Model} -- {Reduce} {All} {Domains} {Into} {One}},
	shorttitle = {{AM}-{RADIO}},
	url = {http://arxiv.org/abs/2312.06709},
	abstract = {A handful of visual foundation models (VFMs) have recently emerged as the backbones for numerous downstream tasks. VFMs like CLIP, DINOv2, SAM are trained with distinct objectives, exhibiting unique characteristics for various downstream tasks. We find that despite their conceptual differences, these models can be effectively merged into a unified model through multi-teacher distillation. We name this approach AM-RADIO (Agglomerative Model -- Reduce All Domains Into One). This integrative approach not only surpasses the performance of individual teacher models but also amalgamates their distinctive features, such as zero-shot vision-language comprehension, detailed pixel-level understanding, and open vocabulary segmentation capabilities. In pursuit of the most hardware-efficient backbone, we evaluated numerous architectures in our multi-teacher distillation pipeline using the same training recipe. This led to the development of a novel architecture (E-RADIO) that exceeds the performance of its predecessors and is at least 7x faster than the teacher models. Our comprehensive benchmarking process covers downstream tasks including ImageNet classification, ADE20k semantic segmentation, COCO object detection and LLaVa-1.5 framework. Code: https://github.com/NVlabs/RADIO},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Ranzinger, Mike and Heinrich, Greg and Kautz, Jan and Molchanov, Pavlo},
	month = apr,
	year = {2024},
	note = {arXiv:2312.06709},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{li_blip-2_2023,
	title = {{BLIP}-2: {Bootstrapping} {Language}-{Image} {Pre}-training with {Frozen} {Image} {Encoders} and {Large} {Language} {Models}},
	shorttitle = {{BLIP}-2},
	url = {http://arxiv.org/abs/2301.12597},
	doi = {10.48550/arXiv.2301.12597},
	abstract = {The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7\% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.},
	urldate = {2024-10-27},
	publisher = {arXiv},
	author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
	month = jun,
	year = {2023},
	note = {arXiv:2301.12597},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}
