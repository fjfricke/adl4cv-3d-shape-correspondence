\section{Introduction}
\label{sec:intro}

The goal of the shape correspondence task is to align two 3D shapes, represented by geometric data (such as point clouds, meshes or voxel grids), to generate a mapping at the region or point level. Most state-of-the-art approaches utilize 2D object detection and semantic segmentation methods under the hood such as \citet{abdelreheem_zero-shot_2023}, utilizing a self-defined combination of Grounding DINO \cite{liu_grounding_2024} for object detection and Segment Anything (SAM) \cite{kirillov_segment_2023} for semantic segmentation. In order to transform 3D object representations into 2D ones, most works typically project the object into multiple images and later back-project them into 3D space. Notable works include \citet{yang_sam3d_2023} (using additional RGB images), \citet{liu_partslip_2022} as well as \citet{abdelreheem_zero-shot_2023} (using multiview renderings), and \citet{cen_segment_2023} (using Neural Radiance Fields). As outlined by \citet{guo_sam2point_2024}, this can be inefficient, lead to a "Degradation of 3D Spacial Information", a "Loss of Prompting Flexibility" and a "Limited Domain Transferability". Therefore, \citet{guo_sam2point_2024} instead propose to represent 3D shapes and scenes as 2D slices following the x-, y- and z-axis which can be mapped back to a voxel representation of the 3D image. The newly published SAM 2 \cite{ravi_sam_2024}, adding video capabilities to SAM alongside advances in Grounding DINO 1.5 \cite{ren_grounding_2024} and the proposed combination of both (Grounded SAM) by \citet{ren_grounded_2024} resp. its successor Grounded SAM 2 make the preservation of 3D shapes during segmentation possible by mimicing a video by voxelation.