\section{Introduction}
\label{sec:intro}

The goal of the shape correspondence task is to align two 3D shapes, represented by geometric data (such as point clouds, meshes or voxel grids), to generate a mapping at the region or point level. Most SOTA approaches often project 3D shapes into multiple 2D views for detection and segmentation under the hood (e.g., with Grounding DINO \cite{liu_grounding_2023} and SAM \cite{kirillov_segment_2023}).
% Most SOTA approaches utilize 2D object detection and semantic segmentation methods under the hood such as \citet{abdelreheem_zero-shot_2023}, utilizing a self-defined combination of Grounding DINO \cite{liu_grounding_2024} for object detection and Segment Anything (SAM) \cite{kirillov_segment_2023} for semantic segmentation.
% To transform 3D object representations into 2D ones, most works typically project the object into multiple images and later backproject them into 3D space.
Notable works include \citet{liu_partslip_2023} as well as \citet{abdelreheem_zero-shot_2023} with SAM-3D (using multiview renderings), and \citet{cen_segment_2023} (using Neural Radiance Fields). As outlined by \citet{guo_sam2point_2024}, this can be inefficient, lead to a "Degradation of 3D Spacial Information", a "Loss of Prompting Flexibility" and a "Limited Domain Transferability". Therefore, \citet{guo_sam2point_2024} instead propose to represent 3D shapes and scenes as 2D slices which can be mapped back to a voxel representation of the 3D image.
The newly published SAM 2 \cite{ravi_sam_2024-1}, adding video capabilities to SAM alongside advances in Grounding DINO 1.5 \cite{ren_grounding_2024} and the proposed combination of both (Grounded SAM) by \citet{ren_grounded_2024} resp. its successor Grounded SAM 2 make the preservation of 3D shapes during segmentation possible by mimicing a video by voxelation.